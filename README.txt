This project uses tensorflow, keras, numpy, scipy, librosa, soundfile, and sklearn.  It takes isolated digits wav files from the TIDIGITS dataset, and trains a deep neural network to predict which digit is being spoken.  Initially, the project does various data augmentation methods on the training set to generate more data for training.  Afterwards, the code trains a deep neural network to predict which digit is being spoken in each wav file.

The entry point for this project is driver.py.  The dataset base directory is hard coded as “../tidigits-isolated-digits-wav/wav/“, so if the TIDIGITS dataset used is stored in a different directory, make sure to set it appropriately in the variable dataset_basedir in driver.py.  This project will create a directory under “../lib/output/“ to store different results.  It will also create a directory within the TIDIGITS dataset, which will be “../tidigits-isolated-digits-wav/wav/train/augmented_data/“.  It will further put subdirectories within the augmented_data folder based on the augmentation type.  Make sure digit sequences are not within TIDIGITS subset, since this project only implemented on isolated digits.